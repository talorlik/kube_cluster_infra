- name: Control Plane EC2s
  hosts: all
  gather_facts: false
  become: yes
  vars:
    ansible_connection: aws_ssm
    ansible_aws_ssm_bucket_name: REPLACE_WITH_BUCKET_NAME
    ansible_aws_ssm_region: REPLACE_WITH_REGION
  pre_tasks:
    - name: Set permissions for /tmp/.ansible
      command: sudo chmod -R 777 /tmp/.ansible
  tasks:
    - name: Get values from environment
      set_fact:
        aws_account: "{{ lookup('env', 'AWS_ACCOUNT') }}"
        aws_region: "{{ lookup('env', 'AWS_REGION') }}"
        certificate_arn: "{{ lookup('env', 'CERTIFICATE_ARN') }}"
        certificate_body_secret_name: "{{ lookup('env', 'CERTIFICATE_BODY_SECRET_NAME') }}"
        certificate_key_secret_name: "{{ lookup('env', 'CERTIFICATE_KEY_SECRET_NAME') }}"
        sub_domain_name: "{{ lookup('env', 'SUB_DOMAIN_NAME') }}"
        alb_sg_id: "{{ lookup('env', 'ALB_SG_ID') }}"
        alb_name: "{{ lookup('env', 'ALB_NAME') }}"
        alb_tags: "{{ lookup('env', 'ALB_TAGS') }}"

    - name: Full post-provision deployment
      shell: |
        sudo -u ubuntu -i <<'EOF'
        ready_nodes=$(kubectl get nodes --no-headers | grep ' Ready' | wc -l)
        total_nodes=$(kubectl get nodes --no-headers | wc -l)
        counter=0
        max_attempts=5

        while true; do
          if [ "$ready_nodes" -ge 2 ] && [ "$ready_nodes" -eq "$total_nodes" ]; then
            echo "Nodes are ready."
            break
          else
            echo "Waiting for nodes..."
            sleep 30

            counter=$((counter + 1))
            if [ "$counter" -ge "$max_attempts" ]; then
                echo "Failed: Nodes are not ready after $max_attempts attempts."
                exit 1
            fi
          fi
        done

        running_pods=$(kubectl get pods -n kube-system --no-headers | grep ' Running' | wc -l)
        total_pods=$(kubectl get pods -n kube-system --no-headers | wc -l)
        counter=0
        max_attempts=5

        while true; do
          if [ "$running_pods" -eq "$total_pods" ]; then
            echo "Pods are ready."
            break
          else
            echo "Waiting on pods..."
            sleep 30

            counter=$((counter + 1))
            if [ "$counter" -ge "$max_attempts" ]; then
                echo "Failed: Pods are not ready after $max_attempts attempts."
                exit 1
            fi
          fi
        done

        kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml

        kubectl apply -k 'github.com/kubernetes/cloud-provider-aws/examples/existing-cluster/base/?ref=master'

        helm repo add aws-ebs-csi-driver https://kubernetes-sigs.github.io/aws-ebs-csi-driver
        helm repo update

        sudo tee /etc/kubernetes/ebs-csi-values.yaml <<EBS_EOF > /dev/null
        storageClasses:
          - name: ebs-sc
            annotations:
              storageclass.kubernetes.io/is-default-class: "true"
            provisioner: ebs.csi.aws.com
            volumeBindingMode: WaitForFirstConsumer
            parameters:
              csi.storage.k8s.io/fstype: xfs
              type: gp3
              encrypted: "true"
        EBS_EOF

        sudo chown ubuntu:ubuntu /etc/kubernetes/ebs-csi-values.yaml
        helm upgrade --install aws-ebs-csi-driver -f /etc/kubernetes/ebs-csi-values.yaml -n kube-system aws-ebs-csi-driver/aws-ebs-csi-driver

        helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
        helm repo update
        helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \
        --namespace ingress-nginx \
        --create-namespace \
        --set controller.service.type=NodePort \
        --set controller.service.nodePorts.https=8443

        kubectl create secret tls tls-secret \
        --namespace ingress-nginx \
        --cert=<(aws secretsmanager get-secret-value --secret-id "{{ certificate_body_secret_name }}" --query SecretString --output text) \
        --key=<(aws secretsmanager get-secret-value --secret-id "{{ certificate_key_secret_name }}" --query SecretString --output text)

        sudo tee /etc/kubernetes/alb-ingress-connect-nginx.yaml <<ING_EOF > /dev/null
        apiVersion: networking.k8s.io/v1
        kind: Ingress
        metadata:
          name: alb-ingress-connect-nginx
          namespace: ingress-nginx
          annotations:
            # Ingress Core Settings
            kubernetes.io/ingress.class: 'alb'
            alb.ingress.kubernetes.io/scheme: 'internet-facing'
            kubernetes.io/service-name: 'ingress-nginx/ingress-nginx-controller'

            # Health Check Settings
            alb.ingress.kubernetes.io/healthcheck-protocol: 'HTTPS'
            alb.ingress.kubernetes.io/healthcheck-port: 8443
            alb.ingress.kubernetes.io/healthcheck-path: '/health'
            alb.ingress.kubernetes.io/healthcheck-interval-seconds: 30
            alb.ingress.kubernetes.io/healthcheck-timeout-seconds: 5
            alb.ingress.kubernetes.io/success-codes: '200-399,404'
            alb.ingress.kubernetes.io/healthy-threshold-count: 5
            alb.ingress.kubernetes.io/unhealthy-threshold-count: 2

            # SSL Settings
            alb.ingress.kubernetes.io/listen-ports: '[{"HTTPS":443}, {"HTTPS":8443}]'
            alb.ingress.kubernetes.io/certificate-arn: '{{ certificate_arn }}'
            nginx.ingress.kubernetes.io/backend-protocol: 'HTTPS'
            nginx.ingress.kubernetes.io/force-ssl-redirect: "true"

            alb.ingress.kubernetes.io/security-groups: '{{ alb_sg_id }}'
            alb.ingress.kubernetes.io/load-balancer-name: '{{ alb_name }}'
            alb.ingress.kubernetes.io/tags: '{{ alb_tags }}'
        spec:
          rules:
          - host: "{{ sub_domain_name }}"
            http:
              paths:
              - path: /dashboard/*
                pathType: Prefix
                backend:
                  service:
                    name: kubernetes-dashboard-kong-proxy
                    port:
                      number: 443
              - path: /*
                pathType: Prefix
                backend:
                  service:
                    name: polybot-service
                    port:
                      number: 8443
          tls:
          - hosts:
            - "{{ sub_domain_name }}"
            secretName: tls-secret
          ingressClassName: nginx
        ING_EOF

        sudo chown ubuntu:ubuntu /etc/kubernetes/alb-ingress-connect-nginx.yaml
        kubectl apply -f /etc/kubernetes/alb-ingress-connect-nginx.yaml

        counter=0
        max_attempts=10

        while [[ $(aws elbv2 describe-load-balancers --names "{{ alb_name }}" --query 'LoadBalancers[0].State.Code' --output text) != "active" ]]; do
          echo "Waiting for ALB to become active..."
          sleep 60

          counter=$((counter + 1))
          if [ "$counter" -ge "$max_attempts" ]; then
              echo "Failed: Nodes are not ready after $max_attempts attempts."
              exit 1
          fi
        done

        alb_dns_name=$(aws elbv2 describe-load-balancers --names "{{ alb_name }}" --query 'LoadBalancers[0].DNSName' --output text)
        alb_zone_id=$(aws elbv2 describe-load-balancers --names "{{ alb_name }}" --query 'LoadBalancers[0].CanonicalHostedZoneId' --output text)
        route53_zone_info=$(aws route53 list-hosted-zones-by-name --dns-name "int-devops.click." --query 'HostedZones[0].Id' --output text)

        aws route53 change-resource-record-sets --hosted-zone-id "$route53_zone_info" --change-batch '
        {
          "Changes": [{
            "Action": "UPSERT",
            "ResourceRecordSet": {
              "Name": "{{ sub_domain_name }}",
              "Type": "A",
              "AliasTarget": {
                "DNSName": "$alb_dns_name",
                "HostedZoneId": "$alb_zone_id",
                "EvaluateTargetHealth": false
              }
            }
          }]
        }'
      EOF