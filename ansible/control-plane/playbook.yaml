- name: Control Plane EC2s
  hosts: all
  become: yes
  become_user: ubuntu
  remote_user: ubuntu
  vars:
    ansible_connection: aws_ssm
    ansible_aws_ssm_bucket_name: REPLACE_WITH_BUCKET_NAME
    ansible_aws_ssm_region: REPLACE_WITH_REGION
  pre_tasks:
    - name: Set permissions for /tmp/.ansible
      command: sudo chmod -R 777 /tmp/.ansible
  tasks:
    - name: Get values from environment
      set_fact:
        aws_account: "{{ lookup('env', 'AWS_ACCOUNT') }}"
        aws_region: "{{ lookup('env', 'AWS_REGION') }}"
        certificate_arn: "{{ lookup('env', 'CERTIFICATE_ARN') }}"
        certificate_body_secret_name: "{{ lookup('env', 'CERTIFICATE_BODY_SECRET_NAME') }}"
        certificate_key_secret_name: "{{ lookup('env', 'CERTIFICATE_KEY_SECRET_NAME') }}"
        sub_domain_name: "{{ lookup('env', 'SUB_DOMAIN_NAME') }}"
        alb_sg_id: "{{ lookup('env', 'ALB_SG_ID') }}"
        alb_name: "{{ lookup('env', 'ALB_NAME') }}"
        alb_tags: "{{ lookup('env', 'ALB_TAGS') }}"

    - name: Ensure all nodes are ready and there are at least two nodes
      shell: |
        export KUBECONFIG=/home/ubuntu/.kube/config
        ready_nodes=$(kubectl get nodes --no-headers | grep ' Ready' | wc -l)
        total_nodes=$(kubectl get nodes --no-headers | wc -l)
        if [ "$ready_nodes" -ge 2 ] && [ "$ready_nodes" -eq "$total_nodes" ]; then
          exit 0
        else
          exit 1
        fi
      register: nodes_check
      retries: 5
      delay: 60
      until: nodes_check.rc == 0

    - name: Ensure kube-system pods are ready
      shell: |
        export KUBECONFIG=/home/ubuntu/.kube/config
        running_pods=$(kubectl get pods -n kube-system --no-headers | grep ' Running' | wc -l)
        total_pods=$(kubectl get pods -n kube-system --no-headers | wc -l)
        if [ "$running_pods" -eq "$total_pods" ]; then
          exit 0
        else
          exit 1
        fi
      register: pods_check
      retries: 5
      delay: 60
      until: pods_check.rc == 0

    - name: Install Container Network Interface (CNI) - Flannel
      shell: |
        export KUBECONFIG=/home/ubuntu/.kube/config
        kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml

    - name: Install AWS Cloud Controller Manager
      shell: |
        export KUBECONFIG=/home/ubuntu/.kube/config
        kubectl apply -k 'github.com/kubernetes/cloud-provider-aws/examples/existing-cluster/base/?ref=master'

    - name: Add helm repository for EBS CSI driver
      shell: |
        export KUBECONFIG=/home/ubuntu/.kube/config
        helm repo add aws-ebs-csi-driver https://kubernetes-sigs.github.io/aws-ebs-csi-driver

    - name: Update helm repositories
      shell: |
        export KUBECONFIG=/home/ubuntu/.kube/config
        helm repo update

    - name: Create EBS CSI driver values file
      become: yes
      become_user: root
      copy:
        content: |
          storageClasses:
            - name: ebs-sc
              annotations:
                storageclass.kubernetes.io/is-default-class: "true"
              provisioner: ebs.csi.aws.com
              volumeBindingMode: WaitForFirstConsumer
              parameters:
                csi.storage.k8s.io/fstype: xfs
                type: gp3
                encrypted: "true"
        dest: /etc/kubernetes/ebs-csi-values.yaml

    - name: Install EBS CSI driver using helm
      shell: |
        export KUBECONFIG=/home/ubuntu/.kube/config
        sudo chown ubuntu:ubuntu /etc/kubernetes/ebs-csi-values.yaml
        helm upgrade --install aws-ebs-csi-driver -f /etc/kubernetes/ebs-csi-values.yaml -n kube-system aws-ebs-csi-driver/aws-ebs-csi-driver

    - name: Add helm repository for nginx ingress controller
      shell: |
        export KUBECONFIG=/home/ubuntu/.kube/config
        helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx

    - name: Update helm repositories (nginx-ingress)
      shell: |
        export KUBECONFIG=/home/ubuntu/.kube/config
        helm repo update

    - name: Install nginx-ingress-controller with NodePort service type
      shell: |
        export KUBECONFIG=/home/ubuntu/.kube/config
        helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx
        --namespace ingress-nginx
        --create-namespace
        --set controller.service.type=NodePort
        --set controller.service.nodePorts.https=8443

    - name: Create the TLS secret using the certificate body and key saved in AWS Secrets Manager
      shell: |
        export KUBECONFIG=/home/ubuntu/.kube/config
        kubectl create secret tls tls-secret
        --namespace ingress-nginx
        --cert=<(aws secretsmanager get-secret-value --secret-id "{{ certificate_body_secret_name }}" --query SecretString --output text)
        --key=<(aws secretsmanager get-secret-value --secret-id "{{ certificate_key_secret_name }}" --query SecretString --output text)

    - name: Create the Ingress resource with ALB configuration
      become: yes
      become_user: root
      copy:
        content: |
          apiVersion: networking.k8s.io/v1
          kind: Ingress
          metadata:
            name: alb-ingress-connect-nginx
            namespace: ingress-nginx
            annotations:
              # Ingress Core Settings
              kubernetes.io/ingress.class: 'alb'
              alb.ingress.kubernetes.io/scheme: 'internet-facing'
              kubernetes.io/service-name: 'ingress-nginx/ingress-nginx-controller'

              # Health Check Settings
              alb.ingress.kubernetes.io/healthcheck-protocol: 'HTTPS'
              alb.ingress.kubernetes.io/healthcheck-port: 8443
              alb.ingress.kubernetes.io/healthcheck-path: '/health'
              alb.ingress.kubernetes.io/healthcheck-interval-seconds: 30
              alb.ingress.kubernetes.io/healthcheck-timeout-seconds: 5
              alb.ingress.kubernetes.io/success-codes: '200-399,404'
              alb.ingress.kubernetes.io/healthy-threshold-count: 5
              alb.ingress.kubernetes.io/unhealthy-threshold-count: 2

              # SSL Settings
              alb.ingress.kubernetes.io/listen-ports: '[{"HTTPS":443}, {"HTTPS":8443}]'
              alb.ingress.kubernetes.io/certificate-arn: '{{ certificate_arn }}'
              nginx.ingress.kubernetes.io/backend-protocol: 'HTTPS'
              nginx.ingress.kubernetes.io/force-ssl-redirect: "true"

              alb.ingress.kubernetes.io/security-groups: '{{ alb_sg_id }}'
              alb.ingress.kubernetes.io/load-balancer-name: '{{ alb_name }}'
              alb.ingress.kubernetes.io/tags: '{{ alb_tags }}'
          spec:
            rules:
            - host: "{{ sub_domain_name }}"
              http:
                paths:
                - path: /dashboard/*
                  pathType: Prefix
                  backend:
                    service:
                      name: kubernetes-dashboard-kong-proxy
                      port:
                        number: 443
                - path: /*
                  pathType: Prefix
                  backend:
                    service:
                      name: polybot-service
                      port:
                        number: 8443
            tls:
            - hosts:
              - "{{ sub_domain_name }}"
              secretName: tls-secret
            ingressClassName: nginx
        dest: /etc/kubernetes/alb-ingress-connect-nginx.yaml

    - name: Apply the Ingress resource
      shell: |
        export KUBECONFIG=/home/ubuntu/.kube/config
        sudo chown ubuntu:ubuntu /etc/kubernetes/alb-ingress-connect-nginx.yaml
        kubectl apply -f /etc/kubernetes/alb-ingress-connect-nginx.yaml

    - name: Wait for the ALB to be created and active using AWS CLI
      shell: |
        while [[ $(aws elbv2 describe-load-balancers --names "{{ alb_name }}" --query 'LoadBalancers[0].State.Code' --output text) != "active" ]]; do
          echo "Waiting for ALB to become active..."
          sleep 60
        done
      retries: 10
      delay: 60

    - name: Set ALB DNS name and Zone ID facts using AWS CLI
      shell: |
        alb_dns_name=$(aws elbv2 describe-load-balancers --names "{{ alb_name }}" --query 'LoadBalancers[0].DNSName' --output text)
        alb_zone_id=$(aws elbv2 describe-load-balancers --names "{{ alb_name }}" --query 'LoadBalancers[0].CanonicalHostedZoneId' --output text)
        echo "alb_dns_name=$alb_dns_name" >> /tmp/ansible_alb_facts
        echo "alb_zone_id=$alb_zone_id" >> /tmp/ansible_alb_facts
      register: alb_info

    - name: Load ALB facts from temporary file
      shell: |
        source /tmp/ansible_alb_facts
        echo "ALB DNS Name: $alb_dns_name"
        echo "ALB Zone ID: $alb_zone_id"
      register: alb_facts

    - name: Get Route53 Zone ID using AWS CLI
      shell: |
        aws route53 list-hosted-zones-by-name --dns-name "int-devops.click." --query 'HostedZones[0].Id' --output text
      register: route53_zone_info

    - name: Create Route53 A record for ALB using AWS CLI
      shell: |
        aws route53 change-resource-record-sets --hosted-zone-id "{{ route53_zone_info.stdout }}" --change-batch '
        {
          "Changes": [{
            "Action": "UPSERT",
            "ResourceRecordSet": {
              "Name": "{{ sub_domain_name }}",
              "Type": "A",
              "AliasTarget": {
                "DNSName": "{{ alb_facts.alb_dns_name }}",
                "HostedZoneId": "{{ alb_facts.alb_zone_id }}",
                "EvaluateTargetHealth": false
              }
            }
          }]
        }'